{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0B04vaUJ8BXAjbFrZzP9A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bye2P0-y_XW4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","# Generate sample data\n","X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","y_kmeans = kmeans.fit_predict(X)\n","\n","# Plot the clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","            s=300, c='red', label='Centroids', marker='X')\n","plt.legend()\n","plt.title(\"K-Means Clustering\")\n","plt.show()\n"]},{"cell_type":"code","source":["from sklearn.cluster import AgglomerativeClustering\n","import scipy.cluster.hierarchy as sch\n","# Create dendrogram\n","plt.figure(figsize=(8, 5))\n","dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n","plt.title(\"Dendrogram\")\n","plt.show()\n","# Apply Agglomerative Clustering\n","# The 'affinity' parameter is deprecated. For 'ward' linkage, 'euclidean' is the default metric.\n","hc = AgglomerativeClustering(n_clusters=4, linkage='ward')\n","y_hc = hc.fit_predict(X)\n","# Plot clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)\n","plt.title(\"Hierarchical Clustering\")\n","plt.show()"],"metadata":{"id":"mD96YsyH_fXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","y_dbscan = dbscan.fit_predict(X)\n","# Plot clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma', alpha=0.7)\n","plt.title(\"DBSCAN Clustering\")\n","plt.show()\n"],"metadata":{"id":"L5k8AesE_llr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.mixture import GaussianMixture\n","# Apply GMM\n","gmm = GaussianMixture(n_components=4, random_state=42)\n","y_gmm = gmm.fit_predict(X)\n","# Plot clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_gmm, cmap='coolwarm', alpha=0.7)\n","plt.title(\"Gaussian Mixture Model Clustering\")\n","plt.show()\n"],"metadata":{"id":"ZBBhEIcU_qFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_digits\n","# Load dataset (Handwritten digits)\n","digits = load_digits()\n","X = digits.data\n","y = digits.target\n","# Apply PCA (Reduce to 2 dimensions)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Scatter plot of PCA results\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n","plt.colorbar(label=\"Digit Label\")\n","plt.title(\"PCA on Handwritten Digits\")\n","plt.show()\n"],"metadata":{"id":"T2WN9cKh_vTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","# Apply t-SNE (Reduce to 2D)\n","tsne = TSNE(n_components=2, random_state=42)\n","X_tsne = tsne.fit_transform(X)\n","# Scatter plot of t-SNE results\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='Spectral', alpha=0.7)\n","plt.colorbar(label=\"Digit Label\")\n","plt.title(\"t-SNE on Handwritten Digits\")\n","plt.show()"],"metadata":{"id":"C0KG1i8I_z6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import umap\n","# Apply UMAP (Reduce to 2D)\n","umap_reducer = umap.UMAP(n_components=2, random_state=42)\n","X_umap = umap_reducer.fit_transform(X)\n","# Scatter plot of UMAP results\n","plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n","plt.colorbar(label=\"Digit Label\")\n","plt.title(\"UMAP on Handwritten Digits\")\n","plt.show()\n"],"metadata":{"id":"TIPddntC_3mL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","# Define Autoencoder Model\n","input_dim = X.shape[1]\n","encoding_dim = 32  # Reduced dimension\n","# Encoder\n","input_layer = keras.layers.Input(shape=(input_dim,))\n","encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n","# Decoder\n","decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n","\n","# Compile Autoencoder\n","autoencoder = keras.models.Model(input_layer, decoded)\n","autoencoder.compile(optimizer='adam', loss='mse')\n","# Train Autoencoder\n","autoencoder.fit(X, X, epochs=20, batch_size=256, shuffle=True, verbose=1)\n","# Get encoded representation\n","encoder = keras.models.Model(input_layer, encoded)\n","X_autoencoded = encoder.predict(X)\n","# Scatter plot of Autoencoder results (first 2 dimensions)\n","plt.scatter(X_autoencoded[:, 0], X_autoencoded[:, 1], c=y, cmap='plasma', alpha=0.7)\n","plt.colorbar(label=\"Digit Label\")\n","plt.title(\"Autoencoder Representation of Digits\")\n","plt.show()"],"metadata":{"id":"lqpFYinJ_-tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n","# Apply K-Means clustering\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","y_kmeans = kmeans.fit_predict(X)\n","# Plot clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', alpha=0.7)\n","# Plot centroids\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label=\"Centroids\")\n","plt.legend()\n","plt.title(\"K-Means Clustering\")\n","plt.show()\n"],"metadata":{"id":"dYy-ACaiAQlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n","# Apply K-Means clustering\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","y_kmeans = kmeans.fit_predict(X)\n","wcss = []  # Within-cluster sum of squares\n","for k in range(1, 11):\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(X)\n","    wcss.append(kmeans.inertia_)\n","\n","# Plot the elbow graph\n","plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n","plt.xlabel(\"Number of Clusters (K)\")\n","plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n","plt.title(\"Elbow Method for Optimal K\")\n","plt.show()\n"],"metadata":{"id":"Nw1ynXF6AYxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.cluster.hierarchy as sch\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.datasets import make_blobs\n","# Generate synthetic data\n","X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n","# Plot the dendrogram\n","plt.figure(figsize=(8, 5))\n","dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n","plt.title(\"Dendrogram\")\n","plt.xlabel(\"Data Points\")\n","plt.ylabel(\"Euclidean Distance\")\n","plt.show()"],"metadata":{"id":"xgwjuQt0AeDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.cluster.hierarchy as sch\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.datasets import make_blobs\n","# Generate synthetic data\n","X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n","# Apply Agglomerative Clustering\n","# The 'affinity' parameter is deprecated. For 'ward' linkage, 'euclidean' is the default metric.\n","hc = AgglomerativeClustering(n_clusters=4, linkage='ward')\n","y_hc = hc.fit_predict(X)\n","# Plot clusters\n","plt.scatter(X[:, 0], X[:, 1], c=y_hc, cmap='rainbow', alpha=0.7)\n","plt.title(\"Hierarchical Clustering\")\n","plt.show()\n","\n","\n"],"metadata":{"id":"GWN27S3lAmrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import StandardScaler\n","# Load the digits dataset\n","digits = load_digits()\n","X = digits.data  # Features (64-dimensional)\n","y = digits.target  # Labels (digits 0-9)\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Apply PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","# Scatter plot of PCA results\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n","plt.colorbar(label=\"Digit Label\")\n","plt.title(\"PCA: Handwritten Digits (2D Projection)\")\n","plt.xlabel(\"Principal Component 1\")\n","plt.ylabel(\"Principal Component 2\")\n","plt.show()\n","# Explained variance ratio\n","explained_variance = pca.explained_variance_ratio_\n","print(f\"Explained Variance (PC1 + PC2): {sum(explained_variance) * 100:.2f}%\")\n","# Compute cumulative explained variance\n","pca_full = PCA().fit(X_scaled)\n","cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n","# Plot cumulative variance\n","plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n","plt.xlabel(\"Number of Principal Components\")\n","plt.ylabel(\"Cumulative Explained Variance\")\n","plt.title(\"PCA: Choosing the Optimal Number of Components\")\n","plt.axhline(y=0.95, color='r', linestyle='--')  # 95% threshold\n","plt.show()\n","optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n","print(f\"Optimal Number of Components for 95% Variance: {optimal_components}\")\n","v"],"metadata":{"id":"NGJSgZC3Arwy"},"execution_count":null,"outputs":[]}]}